# ğŸ§  RL Forex Trading Agent

A complete Reinforcement Learning framework for forex trading with realistic market conditions, dynamic spreads, and risk-management strategies.

## ğŸ¯ Objectives

- Achieve â‰¥70% trade accuracy or $20 daily profit
- Trade with $10 initial capital using 1:2000 leverage
- Implement realistic spread costs and margin requirements

## ğŸš€ Quick Start

1ï¸âƒ£ **Setup Environment**
```bash
python setup.py
```

2ï¸âƒ£ **Test Environment**
```bash
python demo.py
```

3ï¸âƒ£ **Train Agent**
```bash
python train_agent.py
```

## ğŸ“Š Features

### ğŸ§© Trading Environment
| Parameter | Value |
|-----------|-------|
| Initial Balance | $10 USDT |
| Leverage | 1:2000 |
| Spread | $1.6 per lot |
| Min Lot Size | 0.1 |
| Margin | $1 per lot |

### ğŸ§  State Space
- Normalized OHLCV data (20 bars)
- Technical indicators: RSI, MACD, EMA, Bollinger Bands
- Account information: balance, margin, positions
- Performance metrics: win rate, volatility, drawdown

### ğŸ® Action Space
| Action | Description |
|--------|-------------|
| 0 | Hold |
| 1 | Buy (0.1 lot) |
| 2 | Sell (0.1 lot) |
| 3 | Close all positions |

### ğŸ›¡ï¸ Risk Management
- Max drawdown: 20%
- Stop loss: 2% per trade
- Take profit: 4% per trade
- Max concurrent trades: 3
- Trade cooldown: 3â€“5 bars

### ğŸ§¾ Reward Function
- `+5` â†’ profitable trade  
- `-7` â†’ losing trade  
- `-2 Ã— (drawdown / balance)` â†’ penalty  
- `+50` â†’ $20+ daily profit bonus  
- `-20` â†’ $10+ daily loss penalty  

## ğŸ“ˆ Data Sources
- **Primary**: Binance API (ETH/USDT real-time data)
- **Fallback**: Synthetic OHLCV data generator

## ğŸ§  RL Algorithm
| Parameter | Value |
|-----------|-------|
| Algorithm | PPO (Proximal Policy Optimization) |
| Policy | MLP (Multi-Layer Perceptron) |
| Training Steps | 100,000 |
| Evaluation | Every 2,000 steps |

## ğŸ“Š Monitoring & Visualization
- Real-time training progress
- Win rate tracking
- PnL evolution
- Drawdown monitoring
- TensorBoard dashboard

## ğŸ Success Criteria
- âœ… **Accuracy**: â‰¥70% win rate
- âœ… **Profit**: â‰¥$20 daily profit

## ğŸ“ File Structure
```
RL-Trading-strategy/
â”œâ”€â”€ trading_env.py       # Main trading environment
â”œâ”€â”€ train_agent.py       # Training script
â”œâ”€â”€ demo.py              # Environment testing
â”œâ”€â”€ setup.py             # Setup script
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ README.md            # This file
â”œâ”€â”€ models/              # Saved models
â”œâ”€â”€ logs/                # Training logs
â”œâ”€â”€ tensorboard_logs/    # TensorBoard logs
â””â”€â”€ plots/               # Generated plots
```

## âš™ï¸ Configuration
All trading parameters are configurable inside `TradingEnv.__init__()`:
- Initial balance
- Leverage ratio
- Spread costs
- Risk limits
- Reward parameters

## ğŸ“Š Performance Metrics
| Metric | Description |
|--------|-------------|
| Win Rate | % of profitable trades |
| Total PnL | Net profit/loss in USD |
| Sharpe Ratio | Risk-adjusted returns |
| Max Drawdown | Maximum equity decline |
| Final Balance | End-of-episode balance |

## ğŸš¨ Risk Disclaimer
âš ï¸ This project is for educational purposes only.  
Real trading involves significant risk of loss.  
Always use proper risk management and never trade with money you cannot afford to lose.

## ğŸŒŸ Credits
Developed by Muhammad Shahzeb Malik  
ğŸ”— [GitHub Profile](https://github.com/MuhammadShahzebMalik)

## ğŸ§± Optional Enhancements
If you'd like, I can help you add:
- ğŸ“‰ Sample training performance plots
- ğŸ§  Model architecture diagram
- ğŸ”— Shields.io badges (Python, TensorFlow, PPO, License, etc.)
- âš™ï¸ Setup section with Conda / pip commands 
